"""Tests for data quality of generated features.

This module implements data quality checks for features generated by the feature builders.
It ensures that features meet quality standards such as:
1. Having sufficient non-null values
2. Having variability (not all values are the same)
3. Having values within expected ranges
"""

from pathlib import Path

import numpy as np
import polars as pl
import pytest

from src.features.data_quality import (
    BASE_FEATURE_METADATA,
    FeatureQualityChecker,
    get_feature_metadata,
    validate_features,
)

# Test configuration with thresholds
TEST_CONFIG = {
    "validation": {
        "enabled": True,
        "raise_errors": False,
        "abort_on_failure": False,
        "thresholds": {
            "home_win_pct_detailed": 0.5,
            "neutral_win_pct": 0.6,
            "point_diff_stddev": 0.4,
            "scoring_stddev": 0.4,
            "home_court_advantage": 0.5,
            "home_win_boost": 0.5
        }
    }
}

# Feature-specific information for validation - use the same as in the config
FEATURE_METADATA = get_feature_metadata(TEST_CONFIG)


@pytest.fixture
def feature_file_path() -> Path:
    """Get the path to the team performance feature file."""
    # Use the actual feature file path
    file_path = Path("data/features/team_performance.parquet")
    
    # Skip the test if the file doesn't exist
    if not file_path.exists():
        pytest.skip(f"Feature file {file_path} not found")
    
    return file_path


def load_features(file_path: Path) -> pl.DataFrame:
    """Load features from a parquet file."""
    return pl.read_parquet(file_path)


def test_feature_file_exists(feature_file_path: Path) -> None:
    """Test that the feature file exists."""
    assert feature_file_path.exists(), f"Feature file {feature_file_path} does not exist"


def test_feature_columns_exist(feature_file_path: Path) -> None:
    """Test that all expected feature columns exist in the feature file."""
    df = load_features(feature_file_path)
    
    # Get the list of expected feature columns
    expected_features = list(FEATURE_METADATA.keys())
    
    # Ensure all expected features are in the DataFrame
    missing_features = [f for f in expected_features if f not in df.columns]
    assert not missing_features, f"Missing expected features: {missing_features}"


def test_feature_null_percentages(feature_file_path: Path) -> None:
    """Test that the percentage of null values for each feature is below threshold."""
    df = load_features(feature_file_path)
    total_rows = df.height
    
    # Check each feature
    for feature in FEATURE_METADATA:
        if feature not in df.columns:
            pytest.skip(f"Feature {feature} not found in DataFrame")
            
        max_null_pct = FEATURE_METADATA[feature]['max_null_pct']
        null_count = df.select(pl.col(feature).is_null().sum()).item()
        null_pct = null_count / total_rows
        
        assert null_pct <= max_null_pct, (
            f"Feature '{feature}' has {null_pct:.2%} null values, "
            f"which exceeds the threshold of {max_null_pct:.2%}"
        )


def test_feature_variability(feature_file_path: Path) -> None:
    """Test that feature values have variability (not all the same)."""
    df = load_features(feature_file_path)
    
    # Check each feature
    for feature in FEATURE_METADATA:
        if feature not in df.columns:
            pytest.skip(f"Feature {feature} not found in DataFrame")
            
        # Filter out nulls
        non_null_values = df.filter(~pl.col(feature).is_null())
        
        # Skip if no non-null values
        if non_null_values.height == 0:
            continue
            
        # Check uniqueness (at least 2 different values)
        unique_values = non_null_values.select(pl.col(feature).n_unique()).item()
        assert unique_values > 1, f"Feature '{feature}' has no variability (all values are the same)"


def test_feature_value_ranges(feature_file_path: Path) -> None:
    """Test that feature values are within expected ranges."""
    df = load_features(feature_file_path)
    
    # Check each feature
    for feature, metadata in FEATURE_METADATA.items():
        if feature not in df.columns:
            pytest.skip(f"Feature {feature} not found in DataFrame")
            
        # Skip if no range specified
        if 'range' not in metadata or (metadata['range'][0] is None and metadata['range'][1] is None):
            continue
            
        # Filter out nulls and infinities to get valid values
        valid_values = df.filter(
            ~pl.col(feature).is_null() & 
            ~pl.col(feature).is_infinite()
        )
        
        # Skip if no valid values
        if valid_values.height == 0:
            continue
            
        min_val, max_val = metadata['range']
        
        # Check minimum value if specified
        if min_val is not None:
            actual_min = valid_values.select(pl.col(feature).min()).item()
            # Allow for floating point imprecision
            assert actual_min >= min_val - 1e-6, (
                f"Feature '{feature}' has minimum value {actual_min}, "
                f"which is below the expected minimum {min_val}"
            )
        
        # Check maximum value if specified
        if max_val is not None:
            actual_max = valid_values.select(pl.col(feature).max()).item()
            # Allow for floating point imprecision
            assert actual_max <= max_val + 1e-6, (
                f"Feature '{feature}' has maximum value {actual_max}, "
                f"which is above the expected maximum {max_val}"
            )


def test_percentage_features_in_range(feature_file_path: Path) -> None:
    """Test that percentage features are within expected ranges."""
    df = load_features(feature_file_path)
    
    # Test standard percentages (0-1 range)
    standard_percentages = [f for f, m in FEATURE_METADATA.items() 
                          if m.get('type') == 'percentage' and f in df.columns]
    
    for feature in standard_percentages:
        # Get non-null, non-infinite values
        values = df.select(pl.col(feature)).filter(
            ~pl.col(feature).is_null() & 
            ~pl.col(feature).is_infinite()
        )
        if values.height == 0:
            continue
            
        # Check min and max
        min_val = values.min().item()
        max_val = values.max().item()
        
        # Allow for small floating point imprecision
        assert min_val >= -1e-6, f"Feature '{feature}' has values below 0: {min_val}"
        assert max_val <= 1.0 + 1e-6, f"Feature '{feature}' has values above 1: {max_val}"
    
    # Test offset percentages (-1 to 1 range)
    offset_percentages = [f for f, m in FEATURE_METADATA.items() 
                        if m.get('type') == 'offset_percentage' and f in df.columns]
    
    for feature in offset_percentages:
        # Get non-null, non-infinite values
        values = df.select(pl.col(feature)).filter(
            ~pl.col(feature).is_null() & 
            ~pl.col(feature).is_infinite()
        )
        if values.height == 0:
            continue
            
        # Check min and max
        min_val = values.min().item()
        max_val = values.max().item()
        
        # Allow for small floating point imprecision
        assert min_val >= -1.0 - 1e-6, f"Feature '{feature}' has values below -1: {min_val}"
        assert max_val <= 1.0 + 1e-6, f"Feature '{feature}' has values above 1: {max_val}"


def test_feature_distribution_reasonableness(feature_file_path: Path) -> None:
    """Test that feature distributions look reasonable."""
    df = load_features(feature_file_path)
    
    # Test cases for specific features
    
    # Win percentages should have a reasonable distribution
    for feature in ['home_win_pct_detailed', 'away_win_pct_detailed']:
        if feature not in df.columns:
            continue
            
        values = df.select(pl.col(feature)).filter(~pl.col(feature).is_null())
        if values.height == 0:
            continue
            
        # Win percentages should be distributed across the range
        # Check that we have some values in different quartiles
        q1 = values.select(pl.col(feature).quantile(0.25)).item()
        median = values.select(pl.col(feature).median()).item()
        q3 = values.select(pl.col(feature).quantile(0.75)).item()
        
        # There should be some spread in the distribution
        assert q1 < median < q3, (
            f"Feature '{feature}' doesn't have a reasonable distribution. "
            f"Q1={q1}, Median={median}, Q3={q3}"
        )


@pytest.fixture
def sample_good_features() -> pl.DataFrame:
    """Create a sample features DataFrame that should pass all quality checks."""
    # Generate random data within expected ranges
    np.random.seed(42)  # For reproducibility
    
    # Create 100 rows of data with variability and low null rates
    num_rows = 100
    
    return pl.DataFrame({
        # Shooting metrics
        'efg_pct': np.random.uniform(0.3, 0.7, num_rows),
        'ts_pct': np.random.uniform(0.4, 0.8, num_rows),
        'three_point_rate': np.random.uniform(0.1, 0.5, num_rows),
        'ft_rate': np.random.uniform(0.1, 0.6, num_rows),
        
        # Possession metrics
        'orb_pct': np.random.uniform(0.1, 0.4, num_rows),
        'drb_pct': np.random.uniform(0.2, 0.8, num_rows),
        'trb_pct': np.random.uniform(0.3, 0.7, num_rows),
        'ast_rate': np.random.uniform(0.3, 1.5, num_rows),
        'tov_pct': np.random.uniform(0.05, 0.3, num_rows),
        'ast_to_tov_ratio': np.random.uniform(0.5, 2.5, num_rows),
        
        # Win percentage breakdowns
        'home_win_pct_detailed': np.random.uniform(0.3, 0.9, num_rows),
        'away_win_pct_detailed': np.random.uniform(0.2, 0.8, num_rows),
        'neutral_win_pct': np.random.uniform(0.2, 0.8, num_rows - 30).tolist() + [None] * 30,  # 30% nulls for neutral
        'home_games_played': np.random.randint(5, 20, num_rows),
        'away_games_played': np.random.randint(5, 20, num_rows),
        'neutral_games_played': np.random.randint(0, 10, num_rows - 30).tolist() + [None] * 30,  # 30% nulls for neutral
        
        # Form metrics
        'recent_form': np.random.uniform(0.2, 0.9, num_rows - 10).tolist() + [None] * 10,  # 10% nulls
        'scoring_consistency': np.random.uniform(5, 15, num_rows - 10).tolist() + [None] * 10,  # 10% nulls
        'defensive_consistency': np.random.uniform(4, 12, num_rows - 10).tolist() + [None] * 10,  # 10% nulls
        
        # Home court advantage
        'home_court_advantage': np.random.uniform(-5, 15, num_rows - 10).tolist() + [None] * 10,  # 10% nulls
    })


@pytest.fixture
def sample_bad_features() -> pl.DataFrame:
    """Create a sample features DataFrame with quality issues."""
    # Generate data with various quality issues
    num_rows = 100
    
    return pl.DataFrame({
        # Missing column: efg_pct
        
        # Constant values
        'ts_pct': [0.5] * num_rows,
        
        # Out of range
        'three_point_rate': np.random.uniform(0.8, 1.2, num_rows),  # Some values > 1.0
        
        # Too many nulls
        'ft_rate': [None] * 50 + np.random.uniform(0.1, 0.6, num_rows - 50).tolist(),  # 50% nulls
        
        # Other issues
        'orb_pct': [-0.1] * num_rows,  # Negative values
        'drb_pct': np.random.uniform(0.2, 0.8, num_rows),  # This one is good
        'trb_pct': np.random.uniform(0.3, 0.7, num_rows),  # This one is good
    })


def test_validate_features_good(sample_good_features) -> None:
    """Test that validate_features passes for good data."""
    # Create a subset of the metadata for just the columns in our sample
    metadata = {k: v for k, v in BASE_FEATURE_METADATA.items() 
               if k in sample_good_features.columns}
    
    # Adjust thresholds for test data - our sample has 30% nulls for some features
    for feature in metadata:
        if feature in ['neutral_win_pct', 'neutral_games_played']:
            metadata[feature]['max_null_pct'] = 0.4  # Allow 40% nulls for these features
    
    # Should pass validation
    assert validate_features(sample_good_features, metadata) is True


def test_validate_features_bad(sample_bad_features) -> None:
    """Test that validate_features fails for bad data."""
    # Create a subset of the metadata for just the columns in our sample
    metadata = {k: v for k, v in BASE_FEATURE_METADATA.items() 
               if k in sample_bad_features.columns or k == 'efg_pct'}
    
    # Should fail validation
    assert validate_features(sample_bad_features, metadata) is False


def test_feature_quality_checker_missing_columns(sample_bad_features) -> None:
    """Test that the checker detects missing columns."""
    metadata = {'efg_pct': BASE_FEATURE_METADATA['efg_pct']}
    checker = FeatureQualityChecker(metadata)
    
    missing = checker._check_columns_exist(sample_bad_features)
    assert 'efg_pct' in missing


def test_feature_quality_checker_null_percentages(sample_bad_features) -> None:
    """Test that the checker detects too many nulls."""
    # ft_rate has 50% nulls but threshold is 10%
    metadata = {'ft_rate': BASE_FEATURE_METADATA['ft_rate']}
    checker = FeatureQualityChecker(metadata)
    
    null_issues = checker._check_null_percentages(sample_bad_features)
    assert len(null_issues) == 1
    assert 'ft_rate' in null_issues[0]


def test_feature_quality_checker_variability(sample_bad_features) -> None:
    """Test that the checker detects lack of variability."""
    # ts_pct is constant
    metadata = {'ts_pct': BASE_FEATURE_METADATA['ts_pct']}
    checker = FeatureQualityChecker(metadata)
    
    variability_issues = checker._check_variability(sample_bad_features)
    assert len(variability_issues) == 1
    assert 'ts_pct' in variability_issues[0]


def test_feature_quality_checker_ranges(sample_bad_features) -> None:
    """Test that the checker detects out-of-range values."""
    # three_point_rate goes above 1.0
    # orb_pct has negative values
    metadata = {
        'three_point_rate': BASE_FEATURE_METADATA['three_point_rate'],
        'orb_pct': BASE_FEATURE_METADATA['orb_pct']
    }
    checker = FeatureQualityChecker(metadata)
    
    range_issues = checker._check_value_ranges(sample_bad_features)
    assert len(range_issues) == 2
    
    # Check that both issues are detected
    three_point_issue = any('three_point_rate' in issue and 'above' in issue for issue in range_issues)
    orb_issue = any('orb_pct' in issue and 'below' in issue for issue in range_issues)
    
    assert three_point_issue
    assert orb_issue


def test_feature_quality_checker_check_all(sample_bad_features) -> None:
    """Test that check_all correctly aggregates all issues."""
    metadata = {
        'efg_pct': BASE_FEATURE_METADATA['efg_pct'],  # Missing
        'ts_pct': BASE_FEATURE_METADATA['ts_pct'],    # No variability
        'three_point_rate': BASE_FEATURE_METADATA['three_point_rate'],  # Out of range
        'ft_rate': BASE_FEATURE_METADATA['ft_rate'],  # Too many nulls
        'orb_pct': BASE_FEATURE_METADATA['orb_pct'],  # Negative values
    }
    checker = FeatureQualityChecker(metadata)
    
    passed, issues = checker.check_all(sample_bad_features)
    
    assert passed is False
    # Should find 5 issues: missing column, no variability, range issues (2), and too many nulls
    assert len(issues) >= 5


def test_validate_features_with_config(sample_good_features) -> None:
    """Test that validate_features works with configuration-based thresholds."""
    # Get the actual columns in the sample data
    sample_columns = sample_good_features.columns
    
    # Test configuration with custom thresholds - only for columns that exist in the sample
    config = {
        "validation": {
            "thresholds": {
                # Set permissive thresholds for features with high null rates in our sample
                "neutral_win_pct": 0.4,
                "neutral_games_played": 0.4
            }
        }
    }
    
    # Create a custom metadata with only the columns in our sample
    custom_metadata = {k: v for k, v in BASE_FEATURE_METADATA.items() 
                      if k in sample_columns}
    
    # Should pass validation with the custom metadata
    assert validate_features(sample_good_features, feature_metadata=custom_metadata) is True
    
    # Should also pass with config-based validation
    assert validate_features(sample_good_features, config=config) is True
    
    # Create a sample with infinity values
    df_with_inf = sample_good_features.with_columns(pl.lit(float('inf')).alias('inf_feature'))
    
    # Create a new config that includes the infinity feature
    inf_config = {
        "validation": {
            "thresholds": config["validation"]["thresholds"].copy(),
            "infinity_features": ["inf_feature"]
        }
    }
    
    # Should still pass with known infinity features
    assert validate_features(df_with_inf, config=inf_config) is True


def test_get_feature_metadata_from_config() -> None:
    """Test that feature metadata is correctly extracted from configuration."""
    # Create a test configuration with custom thresholds
    config = {
        "validation": {
            "thresholds": {
                "neutral_win_pct": 0.75,
                "home_win_pct_detailed": 0.6,
                "nonexistent_feature": 0.5  # This should be ignored
            }
        }
    }
    
    # Get the metadata from configuration
    metadata = get_feature_metadata(config)
    
    # Check that the thresholds were applied correctly
    assert metadata["neutral_win_pct"]["max_null_pct"] == 0.75
    assert metadata["home_win_pct_detailed"]["max_null_pct"] == 0.6
    
    # Check that other features retain their default thresholds
    assert metadata["efg_pct"]["max_null_pct"] == 0.1
    
    # Check that nonexistent features are ignored
    assert "nonexistent_feature" not in metadata 