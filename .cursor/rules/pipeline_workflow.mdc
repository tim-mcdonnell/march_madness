---
description: Pipeline architecture, execution workflow, and stage interfaces for the NCAA March Madness Predictor. Use when modifying pipeline components or extending the workflow.
globs: ["src/pipeline/*.py", "run_pipeline.py"]
alwaysApply: false
---

# Pipeline Workflow - NCAA March Madness Predictor

## Pipeline Architecture

The pipeline consists of modular stages executed in sequence:

```
Download → Validation → Cleaning → Processing → Feature Engineering → Model Training → Prediction
```

Each component is designed to be extendable with clear interfaces:

- **Loaders**: Download and cache data (`src/data/loader.py`)
- **Validators**: Validate data quality and schemas (`src/data/validation.py`)
- **Cleaners**: Clean and standardize data (`src/data/cleaner.py`)
- **Transformers**: Process and transform data (`src/data/transformer.py`)
- **Feature Engineers**: Create derived features (`src/features/`)
- **Models**: Train and evaluate models (`src/models/`)

## Pipeline Execution

The main entry point is `run_pipeline.py`:

```bash
# Run the full pipeline
python run_pipeline.py

# Run only specific stages
python run_pipeline.py --stages data features

# Process specific years
python run_pipeline.py --years 2023 2024

# Process specific categories
python run_pipeline.py --categories team_box player_box

# Clean data before running
python run_pipeline.py --clean-raw
```

## Data Processing Workflow

1. **Data Collection**: 
   - Raw data is downloaded from sportsdataverse GitHub repository
   - Data is stored in `data/raw/{category}/{year}.parquet`

2. **Data Cleaning**:
   - Raw data is processed to handle missing values, outliers, etc.
   - Cleaned data is stored in `data/processed/{category}/{year}.parquet`

3. **Feature Engineering**:
   - Features are created from processed data
   - Feature data is stored in `data/features/{feature_set}/{year}.parquet`

4. **Model Training**:
   - Models are trained using the engineered features
   - Models are evaluated and saved to `models/`

5. **Prediction Generation**:
   - Trained models are used to generate tournament predictions
   - Visualization and bracket outputs are created

## Separation of Concerns

Maintain clear separation between pipeline stages:

⚠️ **Common Mistake**: Mixing data processing with feature engineering

✅ **Correct Approach**:
- **Data Processing**: Clean, standardize, and organize raw data only
  - Only include metrics directly derivable from the raw data
  - Do not calculate advanced metrics or add synthetic values
  - Store processed data in `data/processed/`

- **Feature Engineering**: Create derived metrics and features separately
  - Calculate advanced metrics like efficiency ratings, strength indicators
  - Apply transformations like normalization, encoding
  - Store engineered features in `data/features/`

## Pipeline Configuration

The pipeline is configured using YAML files in the `config/` directory. The main configuration file is `config/pipeline_config.yaml`:

```yaml
# Data paths and settings
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  features_dir: "data/features"
  models_dir: "models"
  years: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]
  categories: ["play_by_play", "player_box", "schedules", "team_box"]

# Feature engineering settings
features:
  sets: ["basic", "advanced"]
  normalization: true
  
# Model settings
model:
  type: "nn"  # Options: lgbm, nn, ensemble
  params:
    hidden_layers: [64, 32]
    dropout: 0.2
    learning_rate: 0.001
```

## Pipeline Stages

### Data Stage

The data stage handles downloading, validating, and processing the raw data:

```python
# Example of implementing the data stage
def run_data_stage(config):
    """Run the data stage of the pipeline."""
    # Download data
    for category in config.data.categories:
        for year in config.data.years:
            download_data(category, year)
    
    # Validate data
    validate_downloaded_data(config)
    
    # Process data
    process_data(config)
```

### Feature Stage

The feature stage creates derived features from the processed data:

```python
# Example of implementing the feature stage
def run_feature_stage(config):
    """Run the feature engineering stage of the pipeline."""
    for feature_set in config.features.sets:
        for year in config.data.years:
            engineer_features(feature_set, year, config)
```

### Model Stage

The model stage trains and evaluates prediction models:

```python
# Example of implementing the model stage
def run_model_stage(config):
    """Run the model training stage of the pipeline."""
    # Prepare training data
    X_train, X_test, y_train, y_test = prepare_training_data(config)
    
    # Train model
    model_type = config.model.type
    model = train_model(model_type, X_train, y_train, config.model.params)
    
    # Evaluate model
    evaluate_model(model, X_test, y_test)
    
    # Save model
    save_model(model, config.data.models_dir)
```

## Extension Points

The pipeline is designed to be extended at several points:

1. **Data Sources**: Add new data sources in `src/data/loader.py`
2. **Data Validation**: Extend validation in `src/data/validation.py`
3. **Cleaning Methods**: Add new cleaning functions in `src/data/cleaner.py`
4. **Transformations**: Create new transformations in `src/data/transformer.py`
5. **Feature Sets**: Implement new feature builders in `src/features/builders/`
6. **Models**: Add new prediction models in `src/models/`

Each extension should follow the existing patterns and be registered in the appropriate factory classes or configuration files. 