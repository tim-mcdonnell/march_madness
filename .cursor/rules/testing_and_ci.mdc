---
description: Testing strategy, practices, and CI/CD workflows for ensuring code quality. Use when writing tests or understanding the project's quality assurance processes.
globs: ["tests/*.py", ".github/workflows/*.yml"]
alwaysApply: false
---

# Testing and CI/CD - NCAA March Madness Predictor

## Testing Strategy

This project uses a multi-level testing approach:

1. **Unit Tests**: For core functions in the `src` directory
   - Test data processing functions
   - Test model utility functions
   - Test visualization helpers
   - Test EDA script functions

2. **Integration Tests**: For ensuring components work together
   - Test pipeline end-to-end execution
   - Test data flow between pipeline stages
   - Test feature engineering with processed data

3. **Model Evaluation**: For testing predictive models
   - Cross-validation performance metrics
   - Backtesting on historical tournaments
   - Out-of-sample testing

## Test Directory Structure

Tests are organized by component in the `tests/` directory:

```
tests/
├── __init__.py
├── test_data/              # Tests for data processing components
│   ├── __init__.py
│   ├── test_loader.py
│   ├── test_cleaner.py
│   └── test_validator.py
├── test_features/          # Tests for feature engineering
│   ├── __init__.py
│   └── test_builders.py
├── test_models/            # Tests for prediction models
│   ├── __init__.py
│   ├── test_train.py
│   └── test_predict.py
└── test_pipeline/          # Tests for pipeline framework
    ├── __init__.py
    └── test_stages.py
```

## Testing Standards

- Use pytest as the testing framework
- Name test files as `test_*.py` and test functions as `test_*`
- Use pytest fixtures for shared test resources
- Include assertions that verify both function output and side effects

## Test Example

```python
import pytest
import polars as pl
from src.data.cleaner import clean_team_stats

@pytest.fixture
def sample_team_stats():
    """Create a sample team stats DataFrame for testing."""
    return pl.DataFrame({
        "team_id": [1, 2, 3, None],
        "team_name": ["Team A", "Team B", "Team C", "Unknown"],
        "points": [75.5, 68.2, 82.1, None],
        "assists": [12.3, 15.7, None, 10.2]
    })

def test_clean_team_stats(sample_team_stats):
    """Test that team stats cleaning works correctly."""
    # Execute
    result = clean_team_stats(sample_team_stats)
    
    # Assert
    assert result.shape[0] == 3  # Should remove row with null team_id
    assert result["points"].is_not_null().all()  # Points should have no nulls
    assert "team_id" in result.columns
    assert result["team_id"].dtype == pl.Int64
```

## Test Coverage Standards

- Aim for 80%+ code coverage
- Cover all critical paths and edge cases
- Test error handling and edge conditions

## CI/CD Workflows

This project uses GitHub Actions for continuous integration and deployment:

```
.github/workflows/
├── test.yml           # Runs tests on PRs and pushes
├── run_pipeline.yml   # Scheduled pipeline execution
├── badges.yml         # Generates status badges
└── docs.yml           # Builds and deploys documentation
```

## Testing Workflow (`test.yml`)

The testing workflow runs on pull requests and pushes to the main branch:

```yaml
name: Tests

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
      - name: Run linting
        run: |
          ruff check .
      - name: Run tests
        run: |
          pytest --cov=src tests/
      - name: Upload coverage report
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report
          path: .coverage
```

## Pipeline Workflow (`run_pipeline.yml`)

The pipeline workflow runs on a schedule during basketball season:

```yaml
name: Run Pipeline

on:
  schedule:
    - cron: '0 8 * * *'  # Run at 8:00 AM daily
  workflow_dispatch:      # Allow manual triggers

jobs:
  run_pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      - name: Run pipeline
        run: |
          python run_pipeline.py --years 2025 --stages data features model
```

## Common Testing Pitfalls

- ⚠️ Not testing error handling
- ⚠️ Using real data files instead of test fixtures
- ⚠️ Not resetting mutable state between tests
- ⚠️ Hardcoding expected values that might change
- ⚠️ Not testing edge cases
- ⚠️ Writing tests that are too implementation-specific

## Troubleshooting Tests

If a test is failing, consider these common issues:

1. **Data Issues**: Are you using the right test fixtures?
2. **Environment Issues**: Are all dependencies installed?
3. **Path Issues**: Are file paths correct for the test environment?
4. **Race Conditions**: Are tests interfering with each other?
5. **Changed Behavior**: Has the expected behavior changed?

Remember: Tests should be deterministic, isolated, and representative of how code will be used in production. 