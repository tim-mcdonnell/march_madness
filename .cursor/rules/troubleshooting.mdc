---
description: Common issues, error patterns, and troubleshooting steps for the NCAA March Madness Predictor. Reference when encountering errors or unexpected behavior.
globs: ["**/*.py", "**/*.ipynb"]
alwaysApply: false
---

# Troubleshooting Guide

## Common Error Patterns and Solutions

This guide covers frequent issues encountered when working with the NCAA March Madness Predictor and their solutions.

## Data Processing Issues

### Missing Data Files

**Symptoms:**
- FileNotFoundError when attempting to load data
- "No such file or directory" errors
- Pipeline fails at data loading stage

**Troubleshooting Steps:**
1. Check if the raw data files exist in the `data/raw` directory
2. Verify the file path in the data loader configuration
3. Check if data download has been executed:
   ```python
   from src.data.downloader import download_all_data
   download_all_data(force_redownload=True)
   ```
4. Ensure file names match those specified in the loader configuration

**Solutions:**
```python
# Check available data files
import os
data_files = os.listdir("data/raw")
print(data_files)

# Update file paths in configuration if needed
from src.config.config_manager import ConfigManager
config = ConfigManager.load("data/loader.yaml")
config.set("parameters.file_paths.team_box", "data/raw/mbb_team_box_2023.csv")
config.save()
```

### Data Schema Mismatch

**Symptoms:**
- SchemaError or ValueError during data validation
- Errors like "Column X not found in schema" or "Expected type Y, got Z"
- Pipeline fails during data validation

**Troubleshooting Steps:**
1. Examine the schema definition in `src/data/schemas.py`
2. Compare against actual data structure
3. Check if the data source format has changed
4. Verify column names and types match the schema

**Solutions:**
```python
# Print actual schema of data file
import polars as pl
df = pl.read_csv("data/raw/mbb_team_box_2023.csv")
print(df.schema)

# Update schema definition if needed
from src.data.schemas import update_schema
update_schema("team_box", df.schema)
```

### Data Cleaning Failures

**Symptoms:**
- Errors during data cleaning phase
- "Cannot convert X to Y" type errors
- NaN values propagating to later stages

**Troubleshooting Steps:**
1. Identify which cleaning function is failing
2. Check for unexpected data formats or values
3. Examine data for outliers or corrupted entries
4. Look for missing values in key columns

**Solutions:**
```python
# Inspect problematic data
import polars as pl
df = pl.read_parquet("data/interim/problematic_file.parquet")

# Find columns with missing values
missing_counts = df.null_count()
print(missing_counts.filter(pl.col("*") > 0))

# Fix specific cleaning issues
from src.data.cleaning import fix_missing_values
fixed_df = fix_missing_values(df, strategy="mean")
fixed_df.write_parquet("data/interim/fixed_file.parquet")
```

## Feature Engineering Issues

### Feature Calculation Errors

**Symptoms:**
- Errors during feature generation
- "Division by zero" or other mathematical errors
- NaN values in generated features

**Troubleshooting Steps:**
1. Check input data for zeros or NaNs
2. Review the feature calculation logic
3. Verify data is filtered correctly before calculations
4. Check for edge cases in the data

**Solutions:**
```python
# Check for problematic values in input data
import polars as pl
df = pl.read_parquet("data/processed/team_stats.parquet")
print(df.filter(pl.col("possessions") == 0).head())

# Add safeguards to calculations
def safe_offensive_rating(points, possessions):
    """Calculate offensive rating with safeguards."""
    return pl.when(pl.col(possessions) > 0).then(
        (pl.col(points) / pl.col(possessions) * 100)
    ).otherwise(pl.lit(None))

df = df.with_columns([
    safe_offensive_rating("points", "possessions").alias("offensive_rating")
])
```

### Missing Features

**Symptoms:**
- KeyError or missing column errors in modeling stage
- "Feature X not found" errors
- Model training fails due to missing inputs

**Troubleshooting Steps:**
1. Check if all expected features are generated
2. Verify feature builder registration in factory
3. Ensure feature configuration is correct
4. Check if feature selection is filtering out required features

**Solutions:**
```python
# Check available features
import polars as pl
df = pl.read_parquet("data/features/team_features.parquet")
print(df.columns)

# Update feature configuration
from src.config.config_manager import ConfigManager
config = ConfigManager.load("features/builder.yaml")
config.set("parameters.features.sets.team_stats.enabled", True)
config.save()

# Rebuild specific features
from src.features.builder import rebuild_features
rebuild_features(["team_stats"])
```

### Feature Drift

**Symptoms:**
- Model performance degradation
- Distribution of features changes over time
- Unexpected feature values

**Troubleshooting Steps:**
1. Compare feature distributions across seasons
2. Check for changes in underlying data
3. Verify scaling/normalization is consistent
4. Look for changes in feature calculation logic

**Solutions:**
```python
# Analyze feature distributions
import polars as pl
import plotly.express as px

df = pl.read_parquet("data/features/team_features.parquet")
df_agg = (
    df.groupby("season")
    .agg([
        pl.col("offensive_rating").mean().alias("avg_off_rating"),
        pl.col("offensive_rating").std().alias("std_off_rating")
    ])
    .sort("season")
)

# Plot the distribution change
fig = px.line(
    df_agg.to_pandas(),
    x="season",
    y=["avg_off_rating", "std_off_rating"],
    title="Feature Drift Analysis"
)
fig.show()
```

## Model Training Issues

### GPU Memory Errors

**Symptoms:**
- CUDA out of memory errors
- Process killed during training
- System becomes unresponsive

**Troubleshooting Steps:**
1. Check batch size and model size
2. Monitor GPU memory usage
3. Verify other processes aren't using GPU
4. Check for memory leaks in training loop

**Solutions:**
```python
# Reduce batch size
from src.config.config_manager import ConfigManager
config = ConfigManager.load("models/lstm.yaml")
config.set("parameters.training.batch_size", 32)  # Smaller batch size
config.save()

# Use gradient accumulation
def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):
    model.train()
    for i, batch in enumerate(dataloader):
        outputs = model(batch[0])
        loss = loss_fn(outputs, batch[1])
        # Scale the loss
        loss = loss / accumulation_steps
        loss.backward()
        
        # Update weights after accumulation_steps
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
```

### Model Convergence Issues

**Symptoms:**
- Loss doesn't decrease during training
- Validation metrics stagnate or worsen
- Model predicts same value for all inputs

**Troubleshooting Steps:**
1. Check learning rate (too high or too low)
2. Review model architecture for capacity issues
3. Examine feature scaling and normalization
4. Check for data leakage or target leakage
5. Evaluate data quality and class balance

**Solutions:**
```python
# Adjust learning rate with scheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau

def train_with_lr_scheduler(model, dataloader, optimizer, epochs=100):
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    for epoch in range(epochs):
        train_loss = train_epoch(model, dataloader, optimizer)
        val_loss = validate_epoch(model, val_dataloader)
        
        # Schedule the learning rate
        scheduler.step(val_loss)
        
        print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### Overfitting

**Symptoms:**
- Training loss much lower than validation loss
- Model performs well on training data but poorly on test data
- Performance degrades over training epochs

**Troubleshooting Steps:**
1. Implement regularization (L1, L2, dropout)
2. Reduce model complexity
3. Check for data leakage
4. Implement early stopping
5. Use cross-validation

**Solutions:**
```python
# Add regularization
def create_model_with_regularization(input_size, hidden_size, output_size):
    model = torch.nn.Sequential(
        torch.nn.Linear(input_size, hidden_size),
        torch.nn.ReLU(),
        torch.nn.Dropout(0.3),  # Add dropout
        torch.nn.Linear(hidden_size, output_size)
    )
    return model

# Implement early stopping
def train_with_early_stopping(model, dataloader, val_dataloader, optimizer, 
                              patience=10, epochs=100):
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        train_loss = train_epoch(model, dataloader, optimizer)
        val_loss = validate_epoch(model, val_dataloader)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), "models/best_model.pt")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

## Pipeline Issues

### Pipeline Configuration Errors

**Symptoms:**
- Pipeline fails to start or load
- Configuration validation errors
- "Key X not found in configuration" errors

**Troubleshooting Steps:**
1. Check configuration file format and structure
2. Verify all required parameters are present
3. Validate configuration against schema
4. Check for typos in parameter names

**Solutions:**
```python
# Validate configuration manually
from src.config.validator import validate_config
from src.config.config_manager import ConfigManager

config = ConfigManager.load("pipeline/training.yaml")
validation_result = validate_config(config, "pipeline")
print(validation_result)

# Fix common issues
if "missing_keys" in validation_result:
    for key in validation_result["missing_keys"]:
        config.set(key, get_default_value(key))
    config.save()
```

### Data Flow Interruptions

**Symptoms:**
- Pipeline completes but output files are missing
- Specific pipeline stage fails
- Intermediate data files are incomplete

**Troubleshooting Steps:**
1. Check each stage's output files
2. Verify stage dependencies are satisfied
3. Check disk space for output files
4. Examine logs for specific stage failures

**Solutions:**
```python
# Check pipeline stage outputs
import os
from pathlib import Path

def check_pipeline_outputs(pipeline_name):
    """Check outputs for each stage of a pipeline."""
    stages = get_pipeline_stages(pipeline_name)
    results = {}
    
    for stage in stages:
        expected_outputs = get_expected_outputs(stage)
        missing = []
        for output in expected_outputs:
            if not Path(output).exists():
                missing.append(output)
        
        results[stage] = {
            "expected": len(expected_outputs),
            "missing": missing,
            "status": "OK" if not missing else "MISSING"
        }
    
    return results

# Print pipeline status
pipeline_status = check_pipeline_outputs("training")
for stage, status in pipeline_status.items():
    print(f"{stage}: {status['status']} ({len(status['missing'])}/{status['expected']} missing)")
```

## Environment Issues

### Package Dependency Conflicts

**Symptoms:**
- Import errors
- "No module named X" errors
- Version incompatibility warnings

**Troubleshooting Steps:**
1. Check installed package versions
2. Verify requirements match used imports
3. Look for package conflicts
4. Ensure virtual environment is active

**Solutions:**
```python
# Check installed packages
import pkg_resources
installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}
print(installed_packages)

# Update packages to required versions
import subprocess

def install_requirements():
    """Install packages from requirements.txt."""
    subprocess.check_call(["uv", "pip", "install", "-r", "requirements.txt"])

# Create or update requirements.txt
def update_requirements():
    """Update requirements.txt with current package versions."""
    with open("requirements.txt", "w") as f:
        for pkg, version in installed_packages.items():
            f.write(f"{pkg}=={version}\n")
```

### Path and File Permission Issues

**Symptoms:**
- PermissionError when writing files
- "Access denied" errors
- File locking issues

**Troubleshooting Steps:**
1. Check file permissions
2. Verify directory exists before writing
3. Check if files are locked by other processes
4. Use absolute paths instead of relative paths

**Solutions:**
```python
# Ensure directory exists before writing
import os
from pathlib import Path

def ensure_dir(file_path):
    """Make sure directory exists."""
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

# Use with proper path handling
def save_dataframe(df, file_path):
    """Save dataframe with proper path handling."""
    absolute_path = os.path.abspath(file_path)
    ensure_dir(absolute_path)
    df.write_parquet(absolute_path)
```

## Visualization and Reporting Issues

### Visualization Rendering Errors

**Symptoms:**
- Plots not displaying
- "Figure size too large" errors
- Blank or incomplete visualizations

**Troubleshooting Steps:**
1. Check for data issues (NaN, Inf)
2. Verify visualization library compatibility
3. Check figure size and DPI settings
4. Ensure output directory exists

**Solutions:**
```python
# Handle invalid data in visualizations
import numpy as np
import plotly.express as px

def create_safe_visualization(df, x, y, title=""):
    """Create visualization with safeguards for invalid data."""
    # Clean data before plotting
    df = df.copy()
    df = df.filter(
        ~pl.col(x).is_nan() & 
        ~pl.col(y).is_nan() &
        ~pl.col(x).is_infinite() &
        ~pl.col(y).is_infinite()
    )
    
    # Create visualization
    fig = px.scatter(df.to_pandas(), x=x, y=y, title=title)
    return fig
```

### Interactive Dashboard Issues

**Symptoms:**
- Dashboard fails to start
- Components not rendering properly
- Interactivity not working

**Troubleshooting Steps:**
1. Check browser console for JavaScript errors
2. Verify callback functions are properly registered
3. Check if server is running and accessible
4. Inspect network requests for errors

**Solutions:**
```python
# Debug dashboard callbacks
import dash
from dash import dcc, html, callback, Output, Input
import plotly.express as px

def create_debug_dashboard():
    """Create a dashboard with debug information."""
    app = dash.Dash(__name__)
    
    app.layout = html.Div([
        html.H1("Debug Dashboard"),
        html.Div(id="debug-info"),
        dcc.Input(id="test-input", type="text", value="test"),
        dcc.Graph(id="test-graph")
    ])
    
    @app.callback(
        Output("debug-info", "children"),
        Input("test-input", "value")
    )
    def update_debug_info(value):
        try:
            # Log callback execution
            print(f"Callback executed with value: {value}")
            return f"Input received: {value} at {np.datetime64('now')}"
        except Exception as e:
            # Log and display exception
            error_msg = f"Error: {str(e)}"
            print(error_msg)
            return error_msg
    
    @app.callback(
        Output("test-graph", "figure"),
        Input("test-input", "value")
    )
    def update_graph(value):
        try:
            # Create simple test figure
            df = pl.DataFrame({
                "x": range(10),
                "y": [int(c) * i for i, c in enumerate(value + "0" * 10)]
            })
            fig = px.line(df.to_pandas(), x="x", y="y", title=f"Test Graph: {value}")
            return fig
        except Exception as e:
            # Return error figure
            return px.scatter(title=f"Error: {str(e)}")
    
    return app
```

## Debugging Techniques

### Logging

Use logging to track execution flow and debug issues:

```python
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filename='logs/debug.log'
)
logger = logging.getLogger(__name__)

# Use in functions
def function_with_logging():
    logger.info("Function started")
    try:
        # Function code
        result = process_data()
        logger.info(f"Process completed with result: {result}")
        return result
    except Exception as e:
        logger.error(f"Error in function: {str(e)}", exc_info=True)
        raise
```

### Debugging with PDB

Use Python's built-in debugger for interactive debugging:

```python
import pdb

def debug_function():
    # Set breakpoint
    pdb.set_trace()
    
    # Function code
    data = load_data()
    result = process_data(data)
    return result
```

Common PDB commands:
- `n`: Next line
- `s`: Step into function
- `c`: Continue execution
- `p variable`: Print variable value
- `pp variable`: Pretty print variable
- `q`: Quit debugging

### Using Assertions

Add assertions to catch issues early:

```python
def calculate_team_stats(team_df):
    """Calculate team statistics with assertions."""
    # Verify input
    assert not team_df.is_empty(), "Input DataFrame is empty"
    assert "points" in team_df.columns, "Required column 'points' not found"
    
    # Calculate stats
    avg_points = team_df["points"].mean()
    assert not np.isnan(avg_points), "Calculated average points is NaN"
    
    return {
        "avg_points": avg_points,
        "total_games": team_df.height
    }
```

## Contacting Support

If you've tried the troubleshooting steps above and still have issues:

1. Check the project issues on GitHub
2. Create a detailed issue report including:
   - Error messages
   - Steps to reproduce
   - Environment information
   - Code snippets demonstrating the issue
3. Reach out to the project team with your issue link 